{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.1.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from pandas==2.1.1) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from pandas==2.1.1) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from pandas==2.1.1) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from pandas==2.1.1) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.1.1) (1.16.0)\n",
      "Requirement already satisfied: scorecardpy==0.1.9.7 in c:\\users\\faez\\anaconda3\\lib\\site-packages (0.1.9.7)\n",
      "Requirement already satisfied: patsy in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scorecardpy==0.1.9.7) (0.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scorecardpy==0.1.9.7) (3.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scorecardpy==0.1.9.7) (1.22.4)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scorecardpy==0.1.9.7) (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scorecardpy==0.1.9.7) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scorecardpy==0.1.9.7) (1.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->scorecardpy==0.1.9.7) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->scorecardpy==0.1.9.7) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->scorecardpy==0.1.9.7) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->scorecardpy==0.1.9.7) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->scorecardpy==0.1.9.7) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->scorecardpy==0.1.9.7) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->scorecardpy==0.1.9.7) (2.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from matplotlib->scorecardpy==0.1.9.7) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from matplotlib->scorecardpy==0.1.9.7) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from matplotlib->scorecardpy==0.1.9.7) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from matplotlib->scorecardpy==0.1.9.7) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from matplotlib->scorecardpy==0.1.9.7) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\faez\\anaconda3\\lib\\site-packages (from matplotlib->scorecardpy==0.1.9.7) (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==2.1.1\n",
    "!pip install scorecardpy==0.1.9.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, metrics\n",
    "import scorecardpy as sc\n",
    "import pprint\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_data=pd.read_csv('IS453 Group Assignment - Bureau Data.csv')\n",
    "application_data=pd.read_csv('IS453 Group Assignment - Application Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# choosing columns to keep \n",
    "application_data_cleaned = application_data[['SK_ID_CURR', 'TARGET', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
    "                                            'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE',\n",
    "                                            'NAME_INCOME_TYPE', 'DAYS_EMPLOYED', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE',\n",
    "                                            'CNT_FAM_MEMBERS', 'REG_REGION_NOT_LIVE_REGION', 'EXT_SOURCE_1', 'AMT_REQ_CREDIT_BUREAU_MON']]\n",
    "\n",
    "# Now adding CREDIT_CURRENCY to the bureau columns we keep\n",
    "bureau_data_cleaned = bureau_data[['SK_ID_CURR', 'SK_ID_BUREAU', 'CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_DAY_OVERDUE', 'AMT_CREDIT_MAX_OVERDUE',\n",
    "                                  'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_OVERDUE',\n",
    "                                  'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY']]\n",
    "\n",
    "# Filter application data for non-car owners\n",
    "app_filtered = application_data_cleaned[application_data_cleaned['FLAG_OWN_CAR'] == 'N'].copy()\n",
    "app_filtered.drop(['FLAG_OWN_CAR'], axis = 1, inplace = True)\n",
    "# Filter bureau data for currency 1\n",
    "bureau_filtered = bureau_data_cleaned[bureau_data_cleaned['CREDIT_CURRENCY'] == 'currency 1'].copy()\n",
    "bureau_filtered.drop(['CREDIT_CURRENCY'], axis = 1, inplace = True)\n",
    "bureau_filtered = bureau_filtered[bureau_filtered['CREDIT_TYPE'] != 'Car loan']\n",
    "\n",
    "# Aggregating Bureau Data for each application (SK_ID_CURR) - using filtered bureau data\n",
    "bureau_aggregated = bureau_filtered.groupby('SK_ID_CURR').agg({\n",
    "    'SK_ID_BUREAU': 'count',  # Number of past loans\n",
    "    'CREDIT_ACTIVE': lambda x: x.value_counts().idxmax(),  # Most frequent credit status\n",
    "    'CREDIT_DAY_OVERDUE': 'mean',  # Average overdue days\n",
    "    'AMT_CREDIT_MAX_OVERDUE': 'max',  # Max overdue amount\n",
    "    'CNT_CREDIT_PROLONG': 'sum',  # Total prolongations\n",
    "    'AMT_CREDIT_SUM': 'sum',  # Total outstanding credit\n",
    "    'AMT_CREDIT_SUM_DEBT': 'mean',  # Average current debt\n",
    "    'AMT_CREDIT_SUM_OVERDUE': 'max',  # Max overdue amount\n",
    "    'CREDIT_TYPE': lambda x: x.value_counts().idxmax(),  # Most frequent credit type\n",
    "    'DAYS_CREDIT_UPDATE': 'min',  # Most recent credit update\n",
    "    'AMT_ANNUITY': 'mean',  # Average annuity payment\n",
    "}).reset_index()\n",
    "\n",
    "# Feature engineering\n",
    "# Rename SK_ID_BUREAU count column\n",
    "bureau_aggregated.rename(columns={\"SK_ID_BUREAU\": \"CREDIT_COUNT\"}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new feature for overdue credit count (using the filtered bureau data)\n",
    "bureau_aggregated['OVERDUE_CREDIT_COUNT'] = bureau_filtered[bureau_filtered['CREDIT_DAY_OVERDUE'] > 0].groupby('SK_ID_CURR')['SK_ID_BUREAU'].transform('count')\n",
    "\n",
    "# Merge the filtered application data with filtered bureau data\n",
    "merged_data = app_filtered.merge(bureau_aggregated, on='SK_ID_CURR', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# number of rows\n",
    "# rows = merged_data.shape[0]\n",
    "# print(f\"Number of rows after selection: {rows}\")\n",
    "\n",
    "# calculate percentage of bad loans after filtering\n",
    "# total_loans = len(app_filtered)\n",
    "# bad_loans = app_filtered[app_filtered['TARGET'] == 1].shape[0]\n",
    "# print(f\"Number of Bad Loans after selection:{bad_loans}\")\n",
    "# bad_loans_percentage = (bad_loans / total_loans) * 100\n",
    "# print(f\"Percentage of Bad Loans after filtering: {bad_loans_percentage:.2f}%\")\n",
    "\n",
    "# # calculate percentage of bad loans in orig dataset\n",
    "# total_loans = len(application_data_cleaned)\n",
    "# bad_loans = application_data_cleaned[application_data_cleaned['TARGET'] == 1].shape[0]\n",
    "# bad_loans_percentage = (bad_loans / total_loans) * 100\n",
    "# print(f\"Percentage of Bad Loans in orig dataset: {bad_loans_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working data based on week 11 content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk11_working = merged_data.copy()\n",
    "# drop irrelevant columns\n",
    "wk11_working = wk11_working.drop(['FLAG_MOBIL', 'SK_ID_CURR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142046, 26)\n",
      "(60878, 26)\n"
     ]
    }
   ],
   "source": [
    "train, test = sc.split_df(wk11_working, y = 'TARGET', ratio = .7).values()\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating woe binning ...\n",
      "Binning on 142046 rows and 26 columns in 00:00:18\n"
     ]
    }
   ],
   "source": [
    "bins = sc.woebin(train, y = 'TARGET')\n",
    "\n",
    "# for variables, bindetails in bins.items():\n",
    "#     print(variables, \" : \")\n",
    "#     display(bindetails)\n",
    "#     print(\"--\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] converting into woe values ...\n",
      "[INFO] converting into woe values ...\n",
      "             Feature_1            Feature_2  Correlation\n",
      "7  AMT_GOODS_PRICE_woe       AMT_CREDIT_woe     0.875293\n",
      "8       AMT_CREDIT_woe  AMT_GOODS_PRICE_woe     0.875293\n"
     ]
    }
   ],
   "source": [
    "train_woe = sc.woebin_ply(train, bins)\n",
    "test_woe = sc.woebin_ply(test, bins)\n",
    "# drop low IVs (below 0.02)\n",
    "selected_columns = (\n",
    "    'TARGET', 'AMT_CREDIT_SUM_DEBT_woe', 'CREDIT_ACTIVE_woe', 'NAME_INCOME_TYPE_woe',\n",
    "    'EXT_SOURCE_1_woe', 'DAYS_CREDIT_UPDATE_woe', 'AMT_GOODS_PRICE_woe', 'AMT_CREDIT_woe', 'DAYS_EMPLOYED_woe',\n",
    "    'AMT_ANNUITY_x_woe', 'AMT_CREDIT_MAX_OVERDUE_woe'\n",
    ")\n",
    "train_woe = train_woe.loc[:, selected_columns]\n",
    "test_woe = test_woe.loc[:, selected_columns]\n",
    "\n",
    "correlation_matrix = train_woe.corr()\n",
    "correlation_threshold = 0.7\n",
    "\n",
    "# Find highly correlated pairs (excluding self-correlations)\n",
    "high_correlation_pairs = correlation_matrix.where(np.abs(correlation_matrix) > correlation_threshold).stack().reset_index()\n",
    "high_correlation_pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "\n",
    "# Remove duplicate pairs (since correlation is symmetric)\n",
    "high_correlation_pairs = high_correlation_pairs[high_correlation_pairs['Feature_1'] != high_correlation_pairs['Feature_2']]\n",
    "\n",
    "# Display highly correlated pairs\n",
    "print(high_correlation_pairs)\n",
    "\n",
    "# keep amt goods price because higher IV, remove amt_credit\n",
    "selected_columns = (\n",
    "    'TARGET', 'AMT_CREDIT_SUM_DEBT_woe', 'CREDIT_ACTIVE_woe', 'NAME_INCOME_TYPE_woe',\n",
    "    'EXT_SOURCE_1_woe', 'DAYS_CREDIT_UPDATE_woe', 'AMT_GOODS_PRICE_woe', 'DAYS_EMPLOYED_woe',\n",
    "    'AMT_ANNUITY_x_woe', 'AMT_CREDIT_MAX_OVERDUE_woe'\n",
    ")\n",
    "\n",
    "train_woe = train_woe.loc[:, selected_columns]\n",
    "test_woe = test_woe.loc[:, selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                    -0.000239\n",
       "AMT_CREDIT_SUM_DEBT_woe       0.672547\n",
       "CREDIT_ACTIVE_woe             0.397978\n",
       "NAME_INCOME_TYPE_woe          0.368618\n",
       "EXT_SOURCE_1_woe              0.804809\n",
       "DAYS_CREDIT_UPDATE_woe        0.294583\n",
       "AMT_GOODS_PRICE_woe           0.662015\n",
       "DAYS_EMPLOYED_woe             0.597835\n",
       "AMT_ANNUITY_x_woe             0.574368\n",
       "AMT_CREDIT_MAX_OVERDUE_woe    0.934528\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create the X, y parts of data for train and test\n",
    "y_train = train_woe.loc[:, 'TARGET']\n",
    "X_train = train_woe.loc[:, train_woe.columns != 'TARGET']\n",
    "y_test = test_woe.loc[:, 'TARGET']\n",
    "X_test = test_woe.loc[:, train_woe.columns != 'TARGET']\n",
    "\n",
    "# create a logistic regression model object\n",
    "lr = linear_model.LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X_train, y_train)\n",
    "pd.Series(np.concatenate([lr.intercept_, lr.coef_[0]]),\n",
    "          index = np.concatenate([['intercept'], lr.feature_names_in_]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AMT_ANNUITY_x':          variable                bin  points\n",
      "85  AMT_ANNUITY_x            missing    52.0\n",
      "86  AMT_ANNUITY_x     [-inf,16000.0)    60.0\n",
      "87  AMT_ANNUITY_x  [16000.0,24000.0)    56.0\n",
      "88  AMT_ANNUITY_x  [24000.0,42000.0)    55.0\n",
      "89  AMT_ANNUITY_x      [42000.0,inf)    62.0,\n",
      " 'AMT_CREDIT_MAX_OVERDUE':                   variable              bin  points\n",
      "90  AMT_CREDIT_MAX_OVERDUE          missing    56.0\n",
      "91  AMT_CREDIT_MAX_OVERDUE    [-inf,1500.0)    62.0\n",
      "92  AMT_CREDIT_MAX_OVERDUE  [1500.0,5500.0)    50.0\n",
      "93  AMT_CREDIT_MAX_OVERDUE     [5500.0,inf)    51.0,\n",
      " 'AMT_CREDIT_SUM_DEBT':               variable                 bin  points\n",
      "0  AMT_CREDIT_SUM_DEBT             missing    54.0\n",
      "1  AMT_CREDIT_SUM_DEBT      [-inf,20000.0)    64.0\n",
      "2  AMT_CREDIT_SUM_DEBT   [20000.0,80000.0)    57.0\n",
      "3  AMT_CREDIT_SUM_DEBT  [80000.0,300000.0)    52.0\n",
      "4  AMT_CREDIT_SUM_DEBT      [300000.0,inf)    56.0,\n",
      " 'AMT_GOODS_PRICE':            variable                   bin  points\n",
      "66  AMT_GOODS_PRICE               missing    63.0\n",
      "67  AMT_GOODS_PRICE       [-inf,150000.0)    61.0\n",
      "68  AMT_GOODS_PRICE   [150000.0,300000.0)    57.0\n",
      "69  AMT_GOODS_PRICE   [300000.0,500000.0)    52.0\n",
      "70  AMT_GOODS_PRICE   [500000.0,900000.0)    58.0\n",
      "71  AMT_GOODS_PRICE  [900000.0,1100000.0)    63.0\n",
      "72  AMT_GOODS_PRICE       [1100000.0,inf)    67.0,\n",
      " 'CREDIT_ACTIVE':          variable            bin  points\n",
      "9   CREDIT_ACTIVE        missing    55.0\n",
      "10  CREDIT_ACTIVE         Closed    60.0\n",
      "11  CREDIT_ACTIVE  Active%,%Sold    54.0,\n",
      " 'DAYS_CREDIT_UPDATE':               variable               bin  points\n",
      "59  DAYS_CREDIT_UPDATE           missing    55.0\n",
      "60  DAYS_CREDIT_UPDATE    [-inf,-1700.0)    60.0\n",
      "61  DAYS_CREDIT_UPDATE  [-1700.0,-700.0)    58.0\n",
      "62  DAYS_CREDIT_UPDATE    [-700.0,-50.0)    56.0\n",
      "63  DAYS_CREDIT_UPDATE       [-50.0,inf)    54.0,\n",
      " 'DAYS_EMPLOYED':          variable                bin  points\n",
      "79  DAYS_EMPLOYED     [-inf,-4000.0)    66.0\n",
      "80  DAYS_EMPLOYED  [-4000.0,-2000.0)    59.0\n",
      "81  DAYS_EMPLOYED  [-2000.0,-1400.0)    55.0\n",
      "82  DAYS_EMPLOYED      [-1400.0,0.0)    51.0\n",
      "83  DAYS_EMPLOYED          [0.0,inf)    65.0,\n",
      " 'EXT_SOURCE_1':         variable                        bin  points\n",
      "53  EXT_SOURCE_1                    missing    56.0\n",
      "54  EXT_SOURCE_1                [-inf,0.26)    39.0\n",
      "55  EXT_SOURCE_1                [0.26,0.42)    52.0\n",
      "56  EXT_SOURCE_1                [0.42,0.56)    62.0\n",
      "57  EXT_SOURCE_1  [0.56,0.7000000000000001)    69.0\n",
      "58  EXT_SOURCE_1   [0.7000000000000001,inf)    81.0,\n",
      " 'NAME_INCOME_TYPE':             variable                                     bin  points\n",
      "29  NAME_INCOME_TYPE       Businessman%,%Student%,%Pensioner    62.0\n",
      "30  NAME_INCOME_TYPE                           State servant    61.0\n",
      "31  NAME_INCOME_TYPE                    Commercial associate    58.0\n",
      "32  NAME_INCOME_TYPE  Working%,%Maternity leave%,%Unemployed    55.0,\n",
      " 'basepoints':      variable  bin  points\n",
      "0  basepoints  NaN       0}\n"
     ]
    }
   ],
   "source": [
    "card = sc.scorecard(bins, lr, X_train.columns, points0 = 600, odds0 = 1/19, pdo = 20, basepoints_eq0 = True)\n",
    "\n",
    "pprint.pprint(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faez\\anaconda3\\lib\\site-packages\\scorecardpy\\scorecard.py:353: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat_score.loc[:,'score'] = card_basepoints + dat_score.sum(axis=1)\n",
      "C:\\Users\\Faez\\anaconda3\\lib\\site-packages\\scorecardpy\\scorecard.py:353: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat_score.loc[:,'score'] = card_basepoints + dat_score.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "train_score = sc.scorecard_ply(train, card)\n",
    "test_score = sc.scorecard_ply(test, card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model performance at 5:1 odds of default\n",
    "cutoff=560\n",
    "\n",
    "# create sets of predicted bad to compare with actual bad\n",
    "predicted_bad_train = (train_score < cutoff)\n",
    "predicted_bad_train_list = predicted_bad_train.astype(int).values.flatten().tolist()\n",
    "predicted_bad_test = (test_score < cutoff)\n",
    "predicted_bad_test_list = predicted_bad_test.astype(int).values.flatten().tolist()\n",
    "\n",
    "print('*** Training Data Performance ***')\n",
    "print('Confusion matrix:')\n",
    "print(metrics.confusion_matrix(y_train, predicted_bad_train_list))\n",
    "print('PCC measures:')\n",
    "print(metrics.classification_report(y_train, predicted_bad_train_list))\n",
    "\n",
    "print('*** Test Data Performance ***')\n",
    "print('Confusion matrix:')\n",
    "print(metrics.confusion_matrix(y_test, predicted_bad_test_list))\n",
    "print('PCC measures:')\n",
    "print(metrics.classification_report(y_test, predicted_bad_test_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

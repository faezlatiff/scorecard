{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## IS453 Financial Analytics\n",
    "## Week 11 - Credit Scoring Lab Data\n",
    "\n",
    "### Credit risk scorecard construction with scorecardpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## HMEQ Dataset\n",
    "\n",
    "The data set HMEQ reports characteristics and delinquency information for 5,960 home equity loans. A home equity loan is a loan where the obligor uses the equity of his or her home as the underlying collateral. \n",
    "The data is originally taken from the Credit Risk Analytics: Measurement Techniques, Applications, and Examples in SAS book website - https://www.bartbaesens.com/book/6/credit-risk-analytics.\n",
    "A cleaner version of the data is on Kaggle - https://www.kaggle.com/akhil14shukla/loan-defaulter-prediction/data\n",
    "\n",
    "\n",
    "**Variables definition**\n",
    "\n",
    "1. BAD: Binary response variable\n",
    "    - 1 = applicant defaulted on loan or seriously delinquent; \n",
    "    - 0 = applicant paid loan or customer is current on loan payments. This is the class column.\n",
    "2. LOAN: Requested loan amount\n",
    "3. MORTDUE: Amount due on existing mortgage\n",
    "4. VALUE: Value of current property\n",
    "5. REASON: \n",
    "    - DebtCon = debt consolidation(customer uses home equity loan to pay back high interest loans)\n",
    "    - HomeImp = home improvement\n",
    "6. JOB: Occupational categories\n",
    "    - ProfExe\n",
    "    - Mgr\n",
    "    - Office\n",
    "    - Self\n",
    "    - Sales\n",
    "    - Other\n",
    "7. YOJ: Years at present job\n",
    "8. DEROG: Number of major derogatory reports(issued for loans taken in the past when customer fails to keep up the contract or payback on time).\n",
    "9. DELINQ: Number of delinquent credit lines\n",
    "10. CLAGE: Age of oldest credit line in months\n",
    "11. NINQ: Number of recent credit inquiries\n",
    "12. CLNO: Number of credit lines\n",
    "13. DEBTINC: Debt-to-income ratio in percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Install scorecardpy**\n",
    "This is a python version of R package scorecard. The API link has more info :\n",
    "\n",
    "https://pypi.org/project/scorecardpy/\n",
    "\n",
    "https://github.com/shichenxie/scorecardpy/\n",
    "\n",
    "https://cran.r-project.org/web/packages/scorecard/scorecard.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-10-24T09:06:26.678Z",
     "iopub.status.busy": "2023-10-24T09:06:26.644Z",
     "iopub.status.idle": "2023-10-24T09:06:39.779Z",
     "shell.execute_reply": "2023-10-24T09:06:39.807Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# make sure you are running Python 3.9 or later\n",
    "\n",
    "# depending on your environment, either pip install or conda install the following packages\n",
    "# !pip install pandas==2.1.1\n",
    "# !pip install scorecardpy==0.1.9.7\n",
    "\n",
    "# after downloading, restart your kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-10-24T09:06:44.741Z",
     "iopub.status.busy": "2023-10-24T09:06:44.694Z",
     "iopub.status.idle": "2023-04-01T08:02:25.678Z",
     "shell.execute_reply": "2023-04-01T08:02:25.686Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# ignore scorecardpy compatability warnings\n",
    "import warnings\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, metrics\n",
    "import scorecardpy as sc\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the original hmeq_data.csv file**\n",
    "\n",
    "It will have missing values, but that is alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-01T08:02:26.354Z",
     "iopub.status.busy": "2023-04-01T08:02:26.334Z",
     "iopub.status.idle": "2023-04-01T08:02:26.453Z",
     "shell.execute_reply": "2023-04-01T08:02:26.489Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# sample code\n",
    "hmeq_data = pd.read_csv('hmeq_data.csv')\n",
    "\n",
    "# use a copy of hmeq_data for credit risk model\n",
    "hmeq_data_forsc = hmeq_data.copy()\n",
    "\n",
    "# check for missing values\n",
    "hmeq_data_forsc.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Drop MORTDUE, is highly correlated with VALUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-01T08:02:33.853Z",
     "iopub.status.busy": "2023-04-01T08:02:33.844Z",
     "iopub.status.idle": "2023-04-01T08:02:33.870Z",
     "shell.execute_reply": "2023-04-01T08:02:33.881Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "hmeq_data_forsc.drop(columns='MORTDUE', inplace=True)\n",
    "hmeq_data_forsc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do train-test split**\n",
    "\n",
    "`sc.split_df` returns a dictionary of train and test dataset. It uses a fixed random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# split data into 70% train and 30% test\n",
    "train, test = sc.split_df(hmeq_data_forsc, y = 'BAD', ratio = .7).values()\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Generate WOE bins**\n",
    "\n",
    "`sc.woebin()` generates groupings as a python dictionary object and also provides a method to plot WOE for the bins.  It will optimize for IV, but will not attempt to make the trend monotonic.\n",
    "\n",
    "Scorecardpy will automatically do the one-hot encoding as part of the binning process so it is not neccesary to do that in advance.\n",
    "\n",
    "It will also create missing bins for all the variables, so there is no need to imput or remove missing values.\n",
    "\n",
    "*Ignore any Python warning messages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-01T08:02:36.234Z",
     "iopub.status.busy": "2023-04-01T08:02:36.228Z",
     "iopub.status.idle": "2023-03-21T03:38:19.348Z",
     "shell.execute_reply": "2023-03-21T03:38:19.583Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# automatically calculate bin ranges, bins is a dictionary\n",
    "bins = sc.woebin(train, y = 'BAD')\n",
    "\n",
    "for variables, bindetails in bins.items():\n",
    "    print(variables, \" : \")\n",
    "    display(bindetails)\n",
    "    print(\"--\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Logistic regression with WOE encoding\n",
    "\n",
    "Use `sc.woebin_ply` to encode the WOE values\n",
    "\n",
    "Generate the logistic regression model based on the encoded WOE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-21T03:38:52.800Z",
     "iopub.status.busy": "2023-03-21T03:38:52.703Z",
     "iopub.status.idle": "2023-03-21T03:39:07.564Z",
     "shell.execute_reply": "2023-03-21T03:39:08.283Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# prepare a dataset with the WOE values for Logistic Regression training\n",
    "# woebin_ply() converts original values of input data into woe\n",
    "train_woe = sc.woebin_ply(train, bins)\n",
    "test_woe = sc.woebin_ply(test, bins)\n",
    "train_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-21T03:39:07.628Z",
     "iopub.status.busy": "2023-03-21T03:39:07.601Z",
     "iopub.status.idle": "2023-03-21T03:39:07.719Z",
     "shell.execute_reply": "2023-03-21T03:39:08.298Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# create the X, y parts of data for train and test\n",
    "y_train = train_woe.loc[:, 'BAD']\n",
    "X_train = train_woe.loc[:, train_woe.columns != 'BAD']\n",
    "y_test = test_woe.loc[:, 'BAD']\n",
    "X_test = test_woe.loc[:, train_woe.columns != 'BAD']\n",
    "\n",
    "# create a logistic regression model object\n",
    "lr = linear_model.LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X_train, y_train)\n",
    "pd.Series(np.concatenate([lr.intercept_, lr.coef_[0]]),\n",
    "          index = np.concatenate([['intercept'], lr.feature_names_in_]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Generate scorecard\n",
    "\n",
    "Use `sc.scorecard` to generate the scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-21T03:48:23.709Z",
     "iopub.status.busy": "2023-03-21T03:48:23.681Z",
     "iopub.status.idle": "2023-03-21T03:48:23.769Z",
     "shell.execute_reply": "2023-03-21T03:48:23.837Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# generate a card from the model and bins. The scores will be based on probability of default from the model\n",
    "# bins = bins created from sc.woebin\n",
    "# lr = fitted logistic regression model\n",
    "# align target odds with probabity of default = 5%\n",
    "# odds = p/(1-p) = 0.05/(1-0.05) = 0.0526 ~= 1/19\n",
    "card = sc.scorecard(bins, lr, X_train.columns, points0 = 600, odds0 = 1/19, pdo = 20, basepoints_eq0 = True)\n",
    "\n",
    "pprint.pprint(card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex Q1. Calculate the approval status for a new application**\n",
    "\n",
    "Manually calcuate the score and approval status for a cutoff score of 600 and an application with the following information:<BR>\n",
    "- LOAN = 88,900\n",
    "- VALUE = 57,264\n",
    "- REASON = DebtCon\n",
    "- JOB = Other\n",
    "- YOJ = 16.0\n",
    "- DEROG = 0\n",
    "- DELINQ = 0\n",
    "- CLAGE = 221.8\n",
    "- NINQ = 0\n",
    "- CLNO = 16\n",
    "- DEBTINC = 36.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sc.scorecard_ply` to score a new application with the same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# calulate credit score for new application\n",
    "col = ['LOAN','VALUE','REASON','JOB','YOJ','DEROG','DELINQ','CLAGE','NINQ','CLNO','DEBTINC']\n",
    "val = [[88900,57264,'DebtCon','Other',16.0,0.0,0.0,221.8,0.0,16.0,36.1]]\n",
    "new_appl = pd.DataFrame(val, columns = col)\n",
    "\n",
    "new_appl_score = sc.scorecard_ply(new_appl, card, only_total_score = False).transpose()\n",
    "new_appl_score.index = new_appl_score.index.str.replace('_points', '')\n",
    "\n",
    "summary = pd.concat([new_appl.transpose(), new_appl_score], axis=1)\n",
    "summary.columns = ['App Value', 'Points']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score all the test and train data\n",
    "\n",
    "Use `sc.scorecard_ply` to score all the test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# credit score for samples in test and train\n",
    "train_score = sc.scorecard_ply(train, card)\n",
    "test_score = sc.scorecard_ply(test, card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model's performance\n",
    "\n",
    "**Calculate Percentage Correctly Classified measures on the scorecard model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# check model performance at 5:1 odds of default\n",
    "cutoff=560\n",
    "\n",
    "# create sets of predicted bad to compare with actual bad\n",
    "predicted_bad_train = (train_score < cutoff)\n",
    "predicted_bad_train_list = predicted_bad_train.astype(int).values.flatten().tolist()\n",
    "predicted_bad_test = (test_score < cutoff)\n",
    "predicted_bad_test_list = predicted_bad_test.astype(int).values.flatten().tolist()\n",
    "\n",
    "print('*** Training Data Performance ***')\n",
    "print('Confusion matrix:')\n",
    "print(metrics.confusion_matrix(y_train, predicted_bad_train_list))\n",
    "print('PCC measures:')\n",
    "print(metrics.classification_report(y_train, predicted_bad_train_list))\n",
    "\n",
    "print('*** Test Data Performance ***')\n",
    "print('Confusion matrix:')\n",
    "print(metrics.confusion_matrix(y_test, predicted_bad_test_list))\n",
    "print('PCC measures:')\n",
    "print(metrics.classification_report(y_test, predicted_bad_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex Q2. Compare the train vs test model performance**\n",
    "\n",
    "- How do the f1-scores for the training and test dataset compare?\n",
    "- How do the recall and specificity compare?\n",
    "- Does the model appear to be overfitting the training data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate effect of changing the cutoff score\n",
    "\n",
    "Examine the distribution of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine scores for train and test data to assess distribution for entire population\n",
    "combined_score = pd.concat([train_score, test_score], ignore_index=True)\n",
    "\n",
    "# plot distribution of scores on copmbined data\n",
    "combined_score.hist(figsize = (7, 4), bins = 60)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code\n",
    "cutoff = 560\n",
    "\n",
    "approval_count = train_score[train_score[\"score\"]>cutoff].count()['score']\n",
    "approval_rate = approval_count/train_score.shape[0]\n",
    "print(f'Cutoff score of {cutoff:.0f}: {approval_count:,.0f} applications approved ({approval_rate:.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code\n",
    "\n",
    "# calculate expected number of defaults\n",
    "odds_at_cutoff = 5\n",
    "\n",
    "default_prob = 1/(1+odds_at_cutoff)\n",
    "defaults = default_prob*approval_count\n",
    "print(f'Cutoff score of {cutoff:.0f}: {defaults:.0f} defaults expected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex Q3. Evaluate the effect of adjusting the cutoff score**\n",
    "\n",
    "Change the cutoff score to 640\n",
    "- What is the number of applications approved?\n",
    "- What is the number of defaults expected? \n",
    "- How does the recall and specificity performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### DIY\n",
    "\n",
    "**Use scorecardpy for your group assignment**"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
